# vLLM - Understanding vLLM


## What is vLLM

Fastest inference framework to deploy production level llm for inference for high speed inference. It is the fastest llm inference tool for local or self deployed llms. vLLM has exceled in combining all different optimizitations proposed in papers and have unified them to work together. This is done while retaining perfomance of the model.

## Inference Optimizations in vLLM

1. Prefix Caching
    - 
2. Speculative Decoding
    - 
3. Chunked Prefills
    - 
4. Disaggregated Serving
    - 
5. Streaming Prefills
    - 
6. Jump-forward Decording
    - 
7. Quantization 
    - Reducing the precision of the model weights that we store. 
    - FP8, INT8, GPTQ, AWQ
    -  
8. Cascade Attention
    - 
9. Structured Outputs
    - 
10. Cpu KV cache Offloading
    - 
11. Muti-LoRA serving
    - 
